{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3145tttt/arm_gpt_GPT2_88_12_768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "from src.util import set_global_seed\n",
    "from src.eval_promts import clean_text, EVAL_PROMTS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '3145tttt/arm_gpt_GPT2_88_12_768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, max_new_chars=100, temperature=0.1, top_k=100):\n",
    "    set_global_seed(42)\n",
    "    cleaned_prompt = clean_text(prompt)\n",
    "    context = 256\n",
    "    if len(cleaned_prompt) > context:\n",
    "        print(f\"Warning: Cleaned prompt is too long ({len(cleaned_prompt)} > {context} chars). Truncating.\")\n",
    "        cleaned_prompt = cleaned_prompt[-context:]\n",
    "    token_ids = tokenizer.encode(cleaned_prompt)\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    output_ids = model.generate(\n",
    "        input_ids, max_new_tokens=max_new_chars, temperature=temperature,\n",
    "        top_k=top_k, do_sample=True, pad_token_id=3\n",
    "    )\n",
    "    full_output = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
    "    return full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Ով է գրել Եվգենի Օնեգինին:\n",
      "generate: ով է գրել եվգենի օնեգինին: այս աշխատությունը հրատարակվել է 1888 թվականին և առաջին անգամ հրատարակվել է 1889 թվականին\n",
      "\"այս աշխատ\n",
      "==========prompt 2==========\n",
      "prompt: Պուշկինի անունը\n",
      "generate: պուշկինի անունը կրող արձանագրությունները հայտնաբերվել են 19 րդ դարում և հայտնաբերվել են միայն 19 րդ դարում\n",
      "\"այս պու\n",
      "==========prompt 3==========\n",
      "prompt: Երկիրը արեգակնային համակարգի մոլորակ է, օրինակ\n",
      "generate: երկիրը արեգակնային համակարգի մոլորակ է, օրինակ արեգակնային համակարգի մոլորակների մոլորակների մոտ\"\n",
      "\"այս երկիրը բաժանված է երեք մասի արևմտյան և արևե\n",
      "==========prompt 4==========\n",
      "prompt: Լավագույն անիմեն է\n",
      "generate: լավագույն անիմեն է համարվում ամենահայտնի անիմեներից մեկը\n",
      "\"այս անիմեն համարվում է ամենահայտնի անիմեներից մեկը, որը համա\n",
      "==========prompt 5==========\n",
      "prompt: Տիտանիկը հայտնի է նրանով\n",
      "generate: տիտանիկը հայտնի է նրանով, որ այն առաջացել է արաբական աշխարհում այն ժամանակ, երբ այն առաջացել է արաբական աշխարհում այն առաջաց\n",
      "==========prompt 6==========\n",
      "prompt: Ռուսաստանը դա է\n",
      "generate: ռուսաստանը դա է պատճառը, որ այն ամենը, ինչ կարող էր անել, անհայտ է\"\n",
      "\"այս ամենը ռուսաստանի համար անհաջող էր, և նա այ\n",
      "==========prompt 7==========\n",
      "prompt: Հայաստանի լավագույն ուտեստը\n",
      "generate: հայաստանի լավագույն ուտեստը համարվում է աշխարհում ամենահայտնի և ամենահայտնի արտասահմանյան արտադրողներից մեկը\n",
      "\"այս հայաստանի հան\n",
      "==========prompt 8==========\n",
      "prompt: ես հիշում եմ մի հրաշալի պահ\n",
      "generate: ես հիշում եմ մի հրաշալի պահ և այն մասին, որ ես հիշում եմ մի հրաշալի պահ և այն մասին, որ ես հիշում եմ մի հրաշալի պահ ինձ համար\"\n",
      "\n",
      "==========prompt 9==========\n",
      "prompt: Հիմա ծիծաղելի կատակ կլինի,\n",
      "generate: հիմա ծիծաղելի կատակ կլինի, որ նա կարող է այն անվանել անհայտ մարդու անունով\"\n",
      "\"այն բանից հետո, երբ նա արդեն ամուսնացել էր ալեքսա\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(EVAL_PROMTS):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Որտեղ է գտնվում Երևանը? Պատասխան:\",\n",
    "    \"Տոլստոյը գրել է պատերազմ և խաղաղություն? Պատասխան:\",\n",
    "    \"Երևանը Հայաստանի մայրաքաղաքն է? Պատասխան\",\n",
    "    \"ով է գրել 'Եվգենի Օնեգին'?: Պատասխան:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Որտեղ է գտնվում Երևանը? Պատասխան:\n",
      "generate: որտեղ է գտնվում երևանը? պատասխան: այս անգամ արդեն հայտնի է դառնում, որ այն կարող է հանդիսանալ արդեն անհայտ անձի կողմից արդեն անհայտ ա\n",
      "==========prompt 2==========\n",
      "prompt: Տոլստոյը գրել է պատերազմ և խաղաղություն? Պատասխան:\n",
      "generate: տոլստոյը գրել է պատերազմ և խաղաղություն? պատասխան: այս ամենը հանգեցրել է նրան, որ այդ ժամանակ արդեն հայտնի էր որպես արդեն հայտնի արդյունաբերական քաղաք\n",
      "==========prompt 3==========\n",
      "prompt: Երևանը Հայաստանի մայրաքաղաքն է? Պատասխան\n",
      "generate: երևանը հայաստանի մայրաքաղաքն է? պատասխանատու է հայաստանի հանրապետության համար հայաստանի հանրապետության արտակարգ և լիազոր դեսպանի պաշտոնը զբա\n",
      "==========prompt 4==========\n",
      "prompt: ով է գրել 'Եվգենի Օնեգին'?: Պատասխան:\n",
      "generate: ով է գրել 'եվգենի օնեգին'?: պատասխան: այս ամենը հանգեցրեց նրան, որ նա այդ ժամանակ արդեն անցել էր այդ ամենը\"\n",
      "\"այս ամենը հանգեցրեց նրան, որ\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(questions):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Պուշկին - Ալեքսանդր. Տոլստոյ - Լև. Գոգոլ - \n",
      "generate: պուշկին ալեքսանդր. տոլստոյ լև. գոգոլի անվան մրցանակի դափնեկիր (1997)\n",
      "\"արժանացել է ռուսաստանի դաշնության նախագահի կոչման, ռուսաստանի դաշն\n",
      "==========prompt 2==========\n",
      "prompt: Տոլստոյ - Լև. Գոգոլ - Նիկոլայ. Պուշկին - \n",
      "generate: տոլստոյ լև. գոգոլ նիկոլայ. պուշկինի հետ միասին\n",
      "\"այս ամենի հետ մեկտեղ նա հանդիպում է մի աղջկա, ով այդ ժամանակ ապրում էր այդ տանը\"\n",
      "\"այս \n",
      "==========prompt 3==========\n",
      "prompt: Գոգոլ - Նիկոլայ. Պուշկին - Ալեքսանդր. Տոլստոյ - \n",
      "generate: գոգոլ նիկոլայ. պուշկին ալեքսանդր. տոլստոյը հայտարարել է, որ այդ ամենը կարող է լինել անհաջողակ և անհաջողակ\"\n",
      "\"այս ամենը գրեթե անհնար էր, քանի ո\n"
     ]
    }
   ],
   "source": [
    "FEWSHOT = [\n",
    "    \"Պուշկին - Ալեքսանդր. Տոլստոյ - Լև. Գոգոլ - \",\n",
    "    \"Տոլստոյ - Լև. Գոգոլ - Նիկոլայ. Պուշկին - \",\n",
    "    \"Գոգոլ - Նիկոլայ. Պուշկին - Ալեքսանդր. Տոլստոյ - \"\n",
    "]\n",
    "\n",
    "# Пушкин -> Александр, Толстой - Лев, Гоголь - Николая \n",
    "# Николая Гоголь Նիկոլայ Գոգոլ\n",
    "# Александр Пушкин Ալեքսանդր Պուշկին\n",
    "# Лев Толстой Լև Տոլստոյ\n",
    "\n",
    "for i, prompt in enumerate(FEWSHOT):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
