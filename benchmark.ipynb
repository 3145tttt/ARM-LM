{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  3145tttt/arm_gpt_GPT2_88_12_768"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, GPT2LMHeadModel\n",
    "from src.util import set_global_seed\n",
    "from src.eval_promts import clean_text, EVAL_PROMTS\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = '3145tttt/arm_gpt_GPT2_88_12_768'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = GPT2LMHeadModel.from_pretrained(checkpoint)\n",
    "\n",
    "device = 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_prompt(prompt, max_new_chars=100, temperature=0.1, top_k=100):\n",
    "    set_global_seed(42)\n",
    "    cleaned_prompt = clean_text(prompt)\n",
    "    context = 256\n",
    "    if len(cleaned_prompt) > context:\n",
    "        print(f\"Warning: Cleaned prompt is too long ({len(cleaned_prompt)} > {context} chars). Truncating.\")\n",
    "        cleaned_prompt = cleaned_prompt[-context:]\n",
    "    token_ids = tokenizer.encode(cleaned_prompt)\n",
    "    input_ids = torch.tensor([token_ids], dtype=torch.long, device=device)\n",
    "    output_ids = model.generate(\n",
    "        input_ids, max_new_tokens=max_new_chars, temperature=temperature,\n",
    "        top_k=top_k, do_sample=True, pad_token_id=3\n",
    "    )\n",
    "    full_output = tokenizer.decode(output_ids[0].tolist(), skip_special_tokens=True)\n",
    "    return full_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Ով է գրել Եվգենի Օնեգինին:\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(EVAL_PROMTS):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "    \"Որտեղ է գտնվում Երևանը? Պատասխան:\",\n",
    "    \"Տոլստոյը գրել է պատերազմ և խաղաղություն? Պատասխան:\",\n",
    "    \"Երևանը Հայաստանի մայրաքաղաքն է? Պատասխան\",\n",
    "    \"ով է գրել 'Եվգենի Օնեգին'?: Պատասխան:\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Որտեղ է գտնվում Երևանը? Պատասխան:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate: որտեղ է գտնվում երևանը? պատասխան: հետազոտողները կարող են ոչնչացնել հոգին այդ ժամանակ, ինչպես նշել է ռամկավար մոնղոլների հոգին\"\n",
      "\"երկու\n",
      "==========prompt 2==========\n",
      "prompt: Տոլստոյը գրել է պատերազմ և խաղաղություն? Պատասխան:\n",
      "generate: տոլստոյը գրել է պատերազմ և խաղաղություն? պատասխան: հենց այն տեղի է ունեցել մակեդոնիայում\n",
      "\"այս ընթացքում նա առաջին անգամ կատարել է բազմաթիվ գործեր, որո\n",
      "==========prompt 3==========\n",
      "prompt: Երևանը Հայաստանի մայրաքաղաքն է? Պատասխան\n",
      "generate: երևանը հայաստանի մայրաքաղաքն է? պատասխանել է. յ. ղ. շահ.\n",
      "\"մարզադաշտի ընդհանուր վարչական և մշակութային մասը ռանկի, պոմորիեի և արգուշտի շրջանն\n",
      "==========prompt 4==========\n",
      "prompt: ով է գրել 'Եվգենի Օնեգին'?: Պատասխան:\n",
      "generate: ով է գրել 'եվգենի օնեգին'?: պատասխան: հենց այն է նրան, որ հայրը կարդացել է իր ընկեր վիկտորին\"\n",
      "առաջին անգամ կինոնկարն ստացել է 1932 թվական\n"
     ]
    }
   ],
   "source": [
    "for i, prompt in enumerate(questions):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========prompt 1==========\n",
      "prompt: Պուշկին - Ալեքսանդր. Տոլստոյ - Լև. Գոգոլ - \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate: պուշկին ալեքսանդր. տոլստոյ լև. գոգոլի համայնքային կենտրոնը, որտեղ կա նաև մորավայի թանգարանի հավաքածուի մաս\"\n",
      "\"մորավիայի ազատագրումից հետո\n",
      "==========prompt 2==========\n",
      "prompt: Տոլստոյ - Լև. Գոգոլ - Նիկոլայ. Պուշկին - \n",
      "generate: տոլստոյ լև. գոգոլ նիկոլայ. պուշկինի համայնքային կենտրոնը, որտեղ կա նշանավոր այգեգործական կենտրոն, որի անկյունում կառուցված է մուշկայի \n",
      "==========prompt 3==========\n",
      "prompt: Գոգոլ - Նիկոլայ. Պուշկին - Ալեքսանդր. Տոլստոյ - \n",
      "generate: գոգոլ նիկոլայ. պուշկին ալեքսանդր. տոլստոյը համաձայնվեց այս մասին ոչինչ չգտնել — : .\n",
      "\"ըստ գոգոլի խոսքերի, նա արդեն մոռացել է այն փաստը, որ մոտ\n"
     ]
    }
   ],
   "source": [
    "FEWSHOT = [\n",
    "    \"Պուշկին - Ալեքսանդր. Տոլստոյ - Լև. Գոգոլ - \",\n",
    "    \"Տոլստոյ - Լև. Գոգոլ - Նիկոլայ. Պուշկին - \",\n",
    "    \"Գոգոլ - Նիկոլայ. Պուշկին - Ալեքսանդր. Տոլստոյ - \"\n",
    "]\n",
    "\n",
    "# Пушкин -> Александр, Толстой - Лев, Гоголь - Николая \n",
    "# Николая Гоголь Նիկոլայ Գոգոլ\n",
    "# Александр Пушкин Ալեքսանդր Պուշկին\n",
    "# Лев Толстой Լև Տոլստոյ\n",
    "\n",
    "for i, prompt in enumerate(FEWSHOT):\n",
    "    print(f\"==========prompt {i+1}==========\")\n",
    "    print(f\"prompt: {prompt}\")\n",
    "    generated_text = generate_prompt(prompt)\n",
    "    print(f\"generate: {generated_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# :("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
